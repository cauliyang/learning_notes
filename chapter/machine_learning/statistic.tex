\chapter{Probability}\label{chp:Probability}
\minitoc

\section{Basic Concepts}

% https://stanford.edu/~shervine/teaching/cs-229/refresher-probabilities-statistics

Permutation is an arrangement of objects in which the order is important.

\[
	P\left(n,r\right)=  \frac{n!}{\left(n-r\right)!}
\]

Combination is an arrangement of objects in which the order is not important.


\[
	C\left(n,r\right) = \frac{P\left(n,r\right)}{r!} = \frac{n!}{r!\left(n-r\right)!}
\]


in which \(0 \le r \le n\).


% https://github.com/cauliyang/Machine-Learning-Session/blob/master/
\section{Maximum Likelihood Estimation}

\begin{equation}
	\vX = (\vx_1, \vx_2, \dots, \vx_N)\transp, \vx_i = (x_{i1}, x_{i2},\dots, x_{ip})\transp
\end{equation}
in which $N$ is the number of samples, $p$ is the number of features.
The data is sampled from a distribution $p\giventhat{\vx}{\theta}$, where $\theta$ is the parameter of the distribution.


For \(N\)  i.i.d. samples, the likelihood function is \(p \giventhat{\vX}{\theta} = \prod_{i=1}^{N} p \giventhat{\vx_i}{\theta}) \)

In order to get \(\theta\), we use \gls{mle}  to maximize the likelihood function.

\begin{equation}
	\theta_{\mathtt{MLE}} = \argmax_{\theta} \log p\giventhat{\vX}{\theta} = \argmax_{\theta} \sum_{i=1}^{N} \log p\giventhat{\vx_i}{\theta}
\end{equation}

\section{Maximum A Posteriori Estimation}
In Bayes' theorem, the \(\theta\) is not a constant value, but \(\theta \sim  p(\theta) \).
Hence,

\begin{equation}
	p\giventhat{\theta}{\vX} = \frac{p\giventhat{\vX}{\theta} p(\theta)}{p(\vX)}  =  \frac{p\giventhat{\vX}{\theta} p(\theta)}{\int\limits_{\theta} p\giventhat{\vX}{\theta} p(\theta) d\theta}
\end{equation}


In order to get \(\theta\), we use \gls{map}  to maximize the posterior function.

\begin{equation}
	\theta_{\mathtt{MAP}} = \argmax_{\theta} p\giventhat{\theta}{\vX} = \argmax_{\theta} \frac{p\giventhat{\vX}{\theta} p(\theta)}{p(\vX)}
\end{equation}


After \(\theta\) is estimated, then  calculating \(\frac{p\giventhat{\vX}{\theta} \cdot p(\theta)}{\int\limits_{\theta} p\giventhat{\vX}{\theta} p(\theta) d\theta}\) to get the posterior distribution.
We can use the posterior distribution to predict the probability of a new sample \(\vx\).

\begin{equation}
	p \giventhat{x_{\mathtt{new}}}{\vX}  = \int\limits_{\theta} p\giventhat{x_{\mathtt{new}}}{\theta} \cdot p\giventhat{\theta}{\vX} d\theta
\end{equation}

\section{Gaussian Distribution}

Gaussian distribution is also called normal distribution.

\begin{equation}
	\theta = (\mu, \sigma^2), \quad \mu = \frac{1}{N} \sum_{i=1}^{N} x_i, \quad \sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2
\end{equation}

For \gls{mle},

\begin{equation}
	\theta = (\mu, \Sigma) = (\mu, \sigma^2), \quad \theta_{\mathtt{MLE}} = \argmax_{\theta} \log p\giventhat{\vX}{\theta} = \argmax_{\theta} \sum_{i=1}^{N} \log p\giventhat{\vx_i}{\theta}
\end{equation}


Generally, the \gls{pdf} of a Gaussian distribution is:

\begin{equation}
	p\giventhat{x}{\mu, \Sigma} =  \frac{1}{\sqrt{(2\pi)^p \det(\Sigma)}} \exp\left(-\frac{1}{2} (\vx - \mu)\transp \Sigma^{-1} (\vx - \mu)\right)
\end{equation}
in which \(\mu\) is the mean vector, \(\Sigma\) is the covariance matrix, \(\det\) is the determinant of matrix.
\(\det\)  is the product of all eigenvalues of a matrix.

Hence,

\begin{equation}
	\log p\giventhat{\vX}{\theta}  = \sum_{i=1}^{N} \log p\giventhat{x_i}{\theta} = \sum_{i=1}^{N} \log \frac{1}{\sqrt{(2\pi)^p \det(\Sigma)}} \exp\left(-\frac{1}{2} (\vx - \mu)\transp \Sigma^{-1} (\vx - \mu)\right)
\end{equation}

Let's only consider 1 dimension case for brevity, then

\begin{equation}
	\log p\giventhat{\vX}{\theta}  = \sum_{i=1}^{N} \log p\giventhat{x_i}{\theta} = \sum_{i=1}^{N} \log \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{1}{2} \frac{(x - \mu)^2}{\sigma^2}\right)
\end{equation}

Let's get the optimal value for \(\mu\),

\begin{equation}
	\mu_{\mathtt{MLE}} = \argmax_{\mu} \log p\giventhat{\vX}{\theta} = \argmin_{\mu} \sum_{i=1}^{N} \frac{1}{2} \left(x_i - \mu\right)^2
\end{equation}

So,

\begin{equation}
	\frac{\partial \log p\giventhat{\vX}{\theta}}{\partial \mu} = \sum_{i=1}^{N} \left(\mu - x_i\right) = 0 \rightarrow \mu_{\mathtt{MLE}} = \frac{1}{N} \sum_{i=1}^{N} x_i
\end{equation}


Let's get the optimal value for \(\sigma^2\),

\begin{align*}
	\sigma_{\mathtt{MLE}} & = \argmax_{\sigma} \log p\giventhat{\vX}{\theta}                                                                                \\
	                      & =\argmax_{\sigma} \sum_{i=1}^{N} \log \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{1}{2} \frac{(x - \mu)^2}{\sigma^2}\right) \\
	                      & = \argmax_{\sigma} \sum_{i=1}^{N} \left[ - \log\sqrt{2\pi \sigma^2} - \frac{(x - \mu)^2}{2\sigma^2} \right]                     \\
	                      & = \argmin_{\sigma} \sum_{i=1}^N \left[ \log \sigma + \frac{\left(x - \mu\right)^2}{2\sigma^2}\right]                            \\
\end{align*}

Hence,

\begin{equation}
	\frac{\partial}{\partial \sigma} \sum_{i=1}^N \left[\log \sigma + \frac{\left(x_i - \mu\right)^2}{2\sigma^2}\right] = 0 \rightarrow \sigma_{\mathtt{MLE}}^2 = \frac{1}{N} \sum_{i=1}^{N} \left(x_i - \mu\right)^2
\end{equation}

\(\mathbb{E}_{D}\left[\mu_{\mathtt{MLE}}\right]\)  is unbaised.

\begin{equation}
	\mathbb{E}_{D}\left[\mu_{\mathtt{MLE}}\right] =   \mathbb{E}_{D}\left[\frac{1}{N} \sum_{i=1}^{N} x_i\right] = \frac{1}{N} \sum_{i=1}^{N} \mathbb{E}_{D}\left[x_i\right] = \frac{1}{N} \sum_{i=1}^{N} \mu = \mu
\end{equation}

However, \(\mathbb{E}_{D}\left[\sigma_{\mathtt{MLE}}^2\right]\) is biased.

\begin{align}
	\mathbb{E}_{D}\left[\sigma_{\mathtt{MLE}}^2\right] & = \mathbb{E}_{D}\left[\frac{1}{N} \sum_{i=1}^{N} \left(x_i - \mu_{\mathtt{MLE}}\right)^2\right]                                                                                                                                                             \\
	                                                   & = \mathbb{E}_{D}\left[\frac{1}{N} \sum_{i=1}^{N}  \left(x_i - \mu_{\mathtt{MLE}}\right)^2\right]                                                                                                                                                            \\
	                                                   & = \mathbb{E}_{D}\left[\frac{1}{N} \sum_{i=1}^{N}  \left(x_i^2 - 2x_i \mu_{\mathtt{MLE}} + \mu_{\mathtt{MLE}}^2\right) \right] =  \mathbb{E}_{D}\left[\sum_{i=1}^{N} x_i^2 - 2 \frac{1}{N}\sum_{i=1}^{N}x_i \mu_{\mathtt{MLE}} + \mu_{\mathtt{MLE}}^2\right] \\
	                                                   & = \mathbb{E}_{D}\left[\frac{1}{N}\sum_{i=1}^{N} \left(x_i^2 - \mu^2\right) + \mu^2 - \mu_{\mathtt{MLE}}^2 \right]                                                                                                                                           \\
	                                                   & = \sigma^2 - \mathbb{E}_{D} \left[ \mu_{\mathtt{MLE}}^2 - \mu^2\right]                                                                                                                                                                                      \\
	                                                   & = \sigma^2 -  \left(\mathbb{E}_{D} \left[\mu_{\mathtt{MLE}}^2\right] - \mathbb{E}_{D} \left[\mu_{\mathtt{MLE}}^2\right] \right)                                                                                                                             \\
	                                                   & = \sigma^2 -  \mathtt{Var}\left[\mu_{\mathtt{MLE}}\right]  =  \sigma^2 - \mathtt{Var}\left[\frac{1}{N} \sum_{i=1}^N x_i \right]                                                                                                                             \\
	                                                   & = \sigma^2 - \frac{1}{N^2} \sum_{i=1}^N \mathtt{Var}\left[x_i\right] = \frac{N-1}{N} \sigma^2                                                                                                                                                               \\
\end{align}


\section{Bayesian Network} % (fold)
\label{sec:Bayesian Network}

\begin{figure}
	\centering
	\includestandalone[width=0.2\textwidth]{bayesian_network/simple_graph}
	\caption{A simple Bayesian network.}\label{fig:bn_simple_graph}
\end{figure}

% section Bayesian Network (end)

\section{Probability Graph} % (fold)
\label{sec:Probability Graph}

\improvement[inline]{this section is not finished yet. Need to be reviewed p54}

\subsection{Variables Elimination} % (fold)
\label{sub:Variables Elimination}

% subsection Variation Elimination (end)

\subsection{Belief propagation} % (fold)
\label{sub:Belief propagation}

Belief propagation is mainly used for tree data structure, and equals Section~\ref{sub:Variables Elimination} with caching.

\begin{figure}
	\centering
	\includestandalone[width=0.3\textwidth]{bayesian_network/belief_propagation}
	\caption{Belief propagation.}
	\label{fig:belief_propagation}
\end{figure}

% subsection Belief propagation (end)

\subsection{Max-product Algorithm} % (fold)
\label{sub:Max-product Algorithm}

% subsection Max-product Algorithm (end)

\subsection{Factor Graph} % (fold)
\label{sub:Factor Graph}

% subsection Factor Graph (end)


% section Probability Graph (end)

\section{Expectation Maximum} % (fold)
\label{sec:Expectation Maximum}

\[
	\Theta^{\left(t+1\right)} = \argmax_{\Theta} \int_Z \log P \giventhat{x,z}{\theta} \cdot P \giventhat{z}{x, \Theta^{\left(t\right)}} \; dz
\]

\info[inline]{continue on p60}

\section{Gaussian Mixture Model} % (fold)
\label{sec:Gaussian Mixture Model}

\[
	Q\left(\Theta, \Theta^{\left(t\right)}\right) = \int_Z \log P \giventhat{x,z}{\theta} \cdot P \giventhat{z}{x, \Theta^{\left(t\right)}} \; dz
\]

% section Gaussian Mixture Model (end)

% section Epectation Maximum (end)
\section{Hidden Markov Model} % (fold)
\label{sec:Hidden Markov Model}

% section Hidden Markov Model (end)

