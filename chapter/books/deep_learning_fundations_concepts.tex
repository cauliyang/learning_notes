\chapter{Deep Learning Foundations and Concepts}\label{chp:deep_learning_foundations_concepts}
\minitoc

\section{Transformer}

One major advantage of transformers is that transfer learning is very effective, so that a transformer model can be trained on a large body of data and then the trained model can be applied to many downstream tasks using some form of fine-tuning.
A large-scale model that can subsequently be adapted to solve multiple different tasks is known as a \emph{foundation model}~\cite{Bishop2023Nov}.

Furthermore, transformers can be trained in a self-supervised way using unlabelled data, which is especially effective with language models since transformers can exploit vast quantities of text available from the internet and other sources.
The \emph{scaling hypothesis} asserts that simply by increasing the scale of the model, as measured by the number of learnable parameters, and training on a commensurately large data set, significant improvements in performance can be achieved, even with no architectural changes.
Moreover, the transformer is especially well suited to massively parallel processing hardware such as \gls{gpu}, allowing exceptionally large neural network language models having of the order of a trillion (\( 10^{12} \)) parameters to be trained in reasonable time~\cite{Bishop2023Nov}.




\section{\gls{gan}}

